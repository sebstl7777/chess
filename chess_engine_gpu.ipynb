{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a617ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "\n",
    "class ChessEnvironment:\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "        self.state_size = (12, 8, 8)\n",
    "        self.action_size = 8 * 8 * 8 * 8\n",
    "        self.current_player = chess.WHITE\n",
    "\n",
    "    def reset(self, fen=None):\n",
    "        if fen:\n",
    "            self.board.set_fen(fen)\n",
    "        else:\n",
    "            self.board.reset()\n",
    "        self.current_player = self.board.turn\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        state = np.zeros((13, 8, 8), dtype=np.float32)\n",
    "        for square in chess.SQUARES:\n",
    "            piece = self.board.piece_at(square)\n",
    "            if piece is not None:\n",
    "                channel = self._piece_to_channel(piece)\n",
    "                row, col = self._square_to_coords(square)\n",
    "                state[channel, row, col] = 1.0\n",
    "\n",
    "        state[12, :, :] = 1.0 if self.board.turn == chess.WHITE else 0.0\n",
    "\n",
    "        if self.current_player == chess.BLACK:\n",
    "            state = np.flip(state, axis=(1, 2))  # Flip board for Black perspective\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _piece_to_channel(self, piece):\n",
    "        piece_type = piece.piece_type - 1\n",
    "        color_offset = 0 if piece.color == chess.WHITE else 6\n",
    "        return piece_type + color_offset\n",
    "\n",
    "    def _square_to_coords(self, square):\n",
    "        row = 7 - chess.square_rank(square)\n",
    "        col = chess.square_file(square)\n",
    "        return row, col\n",
    "\n",
    "    def get_legal_moves(self):\n",
    "        legal_moves = []\n",
    "        for move in self.board.legal_moves:\n",
    "            row1, col1 = self._square_to_coords(move.from_square)\n",
    "            row2, col2 = self._square_to_coords(move.to_square)\n",
    "            legal_moves.append((row1, col1, row2, col2))\n",
    "        return legal_moves\n",
    "\n",
    "    def step(self, action):\n",
    "        row1, col1, row2, col2 = action\n",
    "        from_square = self._coords_to_square(row1, col1)\n",
    "        to_square = self._coords_to_square(row2, col2)\n",
    "\n",
    "        piece = self.board.piece_at(from_square)\n",
    "        if piece and piece.piece_type == chess.PAWN:\n",
    "            if (piece.color == chess.WHITE and row2 == 0) or (piece.color == chess.BLACK and row2 == 7):\n",
    "                move = chess.Move(from_square, to_square, promotion=chess.QUEEN)\n",
    "            else:\n",
    "                move = chess.Move(from_square, to_square)\n",
    "        else:\n",
    "            move = chess.Move(from_square, to_square)\n",
    "\n",
    "        if move not in self.board.legal_moves:\n",
    "            raise ValueError(f\"Illegal move attempted: {move}\")\n",
    "\n",
    "        self.board.push(move)\n",
    "        new_state = self._get_state()\n",
    "        done = self.board.is_game_over()\n",
    "        reward = self.calculate_reward(done)\n",
    "\n",
    "        return new_state, reward, done\n",
    "\n",
    "    def _coords_to_square(self, row, col):\n",
    "        return chess.square(col, 7 - row)\n",
    "\n",
    "    '''def calculate_reward(env, agent_color):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on the game outcome and the agent's color.\n",
    "        :param env: The ChessEnvironment object.\n",
    "        :param agent_color: The color the agent is playing as (chess.WHITE or chess.BLACK).\n",
    "        :return: The reward for the agent.\n",
    "        \"\"\"\n",
    "        if env.board.is_checkmate():\n",
    "            # Agent wins if it checkmates the opponent\n",
    "            if env.board.turn != agent_color:\n",
    "                return 1.0  # Agent wins\n",
    "            else:\n",
    "                return -1.0  # Agent loses\n",
    "        elif env.board.is_stalemate() or env.board.is_insufficient_material():\n",
    "            return 0.0  # Draw\n",
    "        else:\n",
    "            return 0.0  # Game is still ongoing\n",
    "    '''\n",
    "    \n",
    "    def calculate_reward(env, blah):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on the game outcome.\n",
    "        :param env: The ChessEnvironment object.\n",
    "        :return: A tuple (white_reward, black_reward).\n",
    "        \"\"\"\n",
    "        if env.board.is_checkmate():\n",
    "            # White wins if it's Black's turn (Black is checkmated)\n",
    "            if env.board.turn == chess.BLACK:\n",
    "                return [1.0, -1.0]  # White wins, Black loses\n",
    "            else:\n",
    "                return [-1.0, 1.0]  # Black wins, White loses\n",
    "        elif env.board.is_stalemate() or env.board.is_insufficient_material():\n",
    "            return [0.0, 0.0]  # Draw\n",
    "        else:\n",
    "            return [0.0, 0.0]  # Game is still ongoing\n",
    "           \n",
    "\n",
    "    def render(self):\n",
    "        print(self.board)\n",
    "\n",
    "# Keep everything else unchanged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42dc2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhiteChessCNN(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(WhiteChessCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(13, 32, kernel_size=3, stride=1, padding=1)  # Input: 13 channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))  # Shape: [batch_size, 32, 8, 8]\n",
    "        x = torch.relu(self.conv2(x))  # Shape: [batch_size, 64, 8, 8]\n",
    "        x = x.view(x.size(0), -1)  # Flatten: [batch_size, 64 * 8 * 8]\n",
    "        x = torch.relu(self.fc1(x))  # Shape: [batch_size, 128]\n",
    "        return self.fc2(x)  # Shape: [batch_size, action_size]\n",
    "\n",
    "class BlackChessCNN(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(BlackChessCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(13, 32, kernel_size=3, stride=1, padding=1)  # Input: 13 channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))  # Shape: [batch_size, 32, 8, 8]\n",
    "        x = torch.relu(self.conv2(x))  # Shape: [batch_size, 64, 8, 8]\n",
    "        x = x.view(x.size(0), -1)  # Flatten: [batch_size, 64 * 8 * 8]\n",
    "        x = torch.relu(self.fc1(x))  # Shape: [batch_size, 128]\n",
    "        return self.fc2(x)  # Shape: [batch_size, action_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0bde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, q_network, env, epsilon):\n",
    "    \"\"\"Select an action using epsilon-greedy strategy, ensuring only legal moves are considered.\"\"\"\n",
    "    legal_moves = env.get_legal_moves()  # Get all legal moves as tuples (row1, col1, row2, col2)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        # Exploration: choose a random legal move\n",
    "        return random.choice(legal_moves)\n",
    "    else:\n",
    "        # Exploitation: choose the best legal move using the Q-network\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state).view(8, 8, 8, 8)\n",
    "\n",
    "        # Filter Q-values so only legal moves remain\n",
    "        legal_q_values = []\n",
    "        for move in legal_moves:\n",
    "            row1, col1, row2, col2 = move\n",
    "            legal_q_values.append((move, q_values[row1, col1, row2, col2].item()))\n",
    "\n",
    "        if not legal_q_values:\n",
    "            raise ValueError(\"No legal moves available â€” this shouldn't happen!\")\n",
    "\n",
    "        # Choose the legal move with the highest Q-value\n",
    "        best_move = max(legal_q_values, key=lambda x: x[1])[0]\n",
    "        return best_move\n",
    "\n",
    "def compute_loss(batch, q_network, target_network, gamma):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \"\"\"Compute the loss for a mini-batch of experiences.\"\"\"\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    # Reshape states and next_states to remove extra dimension\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32).squeeze(1).to(device)  # Remove extra dimension\n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32).squeeze(1).to(device)  # Remove extra dimension\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Compute Q-values for current states\n",
    "    q_values = q_network(states)\n",
    "\n",
    "    # Convert actions to a single index for gathering\n",
    "    action_indices = [np.ravel_multi_index(action, (8, 8, 8, 8)) for action in actions]  \n",
    "    # Convert to PyTorch tensor\n",
    "    action_indices = torch.tensor(action_indices, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    # Now gather the Q-values\n",
    "    q_values = q_values.gather(1, action_indices)\n",
    "\n",
    "    # Compute target Q-values\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_network(next_states)\n",
    "        next_q_values = next_q_values.max(dim=1)[0]\n",
    "        targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "    # Compute MSE loss\n",
    "    loss = nn.functional.mse_loss(q_values.squeeze(1), targets)  # squeeze to match dimensions\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7116c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and get a sample state\n",
    "env = ChessEnvironment()\n",
    "\n",
    "# Initialize the CNN\n",
    "action_size = 8 * 8 * 8 * 8  # Maximum number of possible moves\n",
    "\n",
    "white_q_network = WhiteChessCNN(action_size)\n",
    "white_target_network = WhiteChessCNN(action_size)\n",
    "white_target_network.load_state_dict(white_q_network.state_dict())\n",
    "white_target_network.eval()\n",
    "black_q_network = BlackChessCNN(action_size)\n",
    "black_target_network = BlackChessCNN(action_size)\n",
    "black_target_network.load_state_dict(black_q_network.state_dict())\n",
    "black_target_network.eval()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move models to GPU\n",
    "white_q_network.to(device)\n",
    "black_q_network.to(device)\n",
    "white_target_network.to(device)\n",
    "black_target_network.to(device)\n",
    "\n",
    "white_optimizer = optim.Adam(white_q_network.parameters(), lr=0.001)\n",
    "black_optimizer = optim.Adam(black_q_network.parameters(), lr=0.001)\n",
    "white_replay_buffer = deque(maxlen=100000)\n",
    "black_replay_buffer = deque(maxlen=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed5c62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, White Reward: -1.0, Black Reward: 1.0, Epsilon: 0.90\n",
      "Episode: 2, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.89\n",
      "Episode: 3, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.89\n",
      "Episode: 4, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.88\n",
      "Episode: 5, White Reward: 1.0, Black Reward: -1.0, Epsilon: 0.88\n",
      "Episode: 6, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.87\n",
      "Episode: 7, White Reward: -1.0, Black Reward: 1.0, Epsilon: 0.87\n",
      "Episode: 8, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.86\n",
      "Episode: 9, White Reward: -1.0, Black Reward: 1.0, Epsilon: 0.86\n",
      "Episode: 10, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.86\n",
      "Episode: 11, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.85\n",
      "Episode: 12, White Reward: -1.0, Black Reward: 1.0, Epsilon: 0.85\n",
      "Episode: 13, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.84\n",
      "Episode: 14, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.84\n",
      "Episode: 15, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.83\n",
      "Episode: 16, White Reward: 1.0, Black Reward: -1.0, Epsilon: 0.83\n",
      "Episode: 17, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.83\n",
      "Episode: 18, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.82\n",
      "Episode: 19, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.82\n",
      "Episode: 20, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.81\n",
      "Episode: 21, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.81\n",
      "Episode: 22, White Reward: 1.0, Black Reward: -1.0, Epsilon: 0.81\n",
      "Episode: 23, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.80\n",
      "Episode: 24, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.80\n",
      "Episode: 25, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.79\n",
      "Episode: 26, White Reward: 1.0, Black Reward: -1.0, Epsilon: 0.79\n",
      "Episode: 27, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.79\n",
      "Episode: 28, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.78\n",
      "Episode: 29, White Reward: -1.0, Black Reward: 1.0, Epsilon: 0.78\n",
      "Episode: 30, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.77\n",
      "Episode: 31, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.77\n",
      "Episode: 32, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.77\n",
      "Episode: 33, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.76\n",
      "Episode: 34, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.76\n",
      "Episode: 35, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.76\n",
      "Episode: 36, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.75\n",
      "Episode: 37, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.75\n",
      "Episode: 38, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.74\n",
      "Episode: 39, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.74\n",
      "Episode: 40, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.74\n",
      "Episode: 41, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.73\n",
      "Episode: 42, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.73\n",
      "Episode: 43, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.73\n",
      "Episode: 44, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.72\n",
      "Episode: 45, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.72\n",
      "Episode: 46, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.71\n",
      "Episode: 47, White Reward: 0.0, Black Reward: 0.0, Epsilon: 0.71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mturn \u001b[38;5;241m==\u001b[39m chess\u001b[38;5;241m.\u001b[39mWHITE:\n\u001b[1;32m     26\u001b[0m     action \u001b[38;5;241m=\u001b[39m select_action(torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device), white_q_network, env, epsilon)\n\u001b[0;32m---> 27\u001b[0m     next_state, _, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Immediate reward is 0 for non-terminal states\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(next_state, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Store experience in White's replay buffer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 76\u001b[0m, in \u001b[0;36mChessEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIllegal move attempted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmove\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mpush(move)\n\u001b[0;32m---> 76\u001b[0m new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mis_game_over()\n\u001b[1;32m     78\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_reward(done)\n",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mChessEnvironment._get_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m square \u001b[38;5;129;01min\u001b[39;00m chess\u001b[38;5;241m.\u001b[39mSQUARES:\n\u001b[0;32m---> 27\u001b[0m     piece \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpiece_at\u001b[49m\u001b[43m(\u001b[49m\u001b[43msquare\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m piece \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         channel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_piece_to_channel(piece)\n",
      "File \u001b[0;32m~/.conda/envs/stl7/lib/python3.12/site-packages/chess/__init__.py:830\u001b[0m, in \u001b[0;36mBaseBoard.piece_at\u001b[0;34m(self, square)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m piece_type:\n\u001b[1;32m    829\u001b[0m     mask \u001b[38;5;241m=\u001b[39m BB_SQUARES[square]\n\u001b[0;32m--> 830\u001b[0m     color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moccupied_co\u001b[49m\u001b[43m[\u001b[49m\u001b[43mWHITE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Piece(piece_type, color)\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilon = .9  # Initial exploration rate\n",
    "epsilon_min = 0.01  # Minimum exploration rate\n",
    "epsilon_decay = 0.995  # Decay rate for exploration\n",
    "gamma = 0.99  # Discount factor\n",
    "batch_size = 64  # Mini-batch size\n",
    "target_update_freq = 100  # Frequency of updating the target network\n",
    "num_episodes = 100  # Number of episodes to train\n",
    "queens_gambit_fen = \"rnbqkbnr/ppp1pppp/8/3p4/2PP4/8/PP2PPPP/RNBQKBNR w KQkq - 0 3\"\n",
    "ruy_lopez_fen = \"r1bqkbnr/pppp1ppp/2n5/1B2p3/4P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 4\"\n",
    "italian_game_fen = \"r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 4\"\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "for episode in range(num_episodes):\n",
    "    env = ChessEnvironment()\n",
    "    state = env.reset(fen=italian_game_fen)\n",
    "    state = np.expand_dims(state, axis=0).copy()  # Add batch dimension\n",
    "    done = False\n",
    "    white_total_reward = 0\n",
    "    black_total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # White's turn\n",
    "        if env.board.turn == chess.WHITE:\n",
    "            action = select_action(torch.tensor(state, dtype=torch.float32).to(device), white_q_network, env, epsilon)\n",
    "            next_state, _, done = env.step(action)  # Immediate reward is 0 for non-terminal states\n",
    "            next_state = np.expand_dims(next_state, axis=0).copy()  # Add batch dimension\n",
    "\n",
    "            # Store experience in White's replay buffer\n",
    "            white_replay_buffer.append((state, action, 0.0, next_state, done))  # Immediate reward is 0\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "        # Black's turn\n",
    "        else:\n",
    "            action = select_action(torch.tensor(state, dtype=torch.float32).to(device), black_q_network, env, epsilon)\n",
    "            next_state, _, done = env.step(action)  # Immediate reward is 0 for non-terminal states\n",
    "            next_state = np.expand_dims(next_state, axis=0).copy()  # Add batch dimension\n",
    "\n",
    "            # Store experience in Black's replay buffer\n",
    "            black_replay_buffer.append((state, action, 0.0, next_state, done))  # Immediate reward is 0\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "    # Calculate the reward at the end of the game\n",
    "    if env.board.is_checkmate():\n",
    "        if env.board.turn == chess.BLACK:\n",
    "            white_reward, black_reward = 1.0, -1.0  # White wins, Black loses\n",
    "        else:\n",
    "            white_reward, black_reward = -1.0, 1.0  # Black wins, White loses\n",
    "    elif env.board.is_stalemate() or env.board.is_insufficient_material():\n",
    "        white_reward, black_reward = 0.0, 0.0  # Draw\n",
    "    else:\n",
    "        white_reward, black_reward = 0.0, 0.0  # Game is still ongoing (should not happen)\n",
    "\n",
    "    # Update the rewards in the replay buffers for terminal states\n",
    "    for i in range(len(white_replay_buffer)):\n",
    "        state, action, _, next_state, done = white_replay_buffer[i]\n",
    "        if done:\n",
    "            white_replay_buffer[i] = (state, action, white_reward, next_state, done)\n",
    "\n",
    "    for i in range(len(black_replay_buffer)):\n",
    "        state, action, _, next_state, done = black_replay_buffer[i]\n",
    "        if done:\n",
    "            black_replay_buffer[i] = (state, action, black_reward, next_state, done)\n",
    "\n",
    "    # Update total rewards\n",
    "    white_total_reward += white_reward\n",
    "    black_total_reward += black_reward\n",
    "\n",
    "    # Train the White model\n",
    "    if len(white_replay_buffer) > batch_size:\n",
    "        batch = random.sample(white_replay_buffer, batch_size)\n",
    "        loss = compute_loss(batch, white_q_network, white_target_network, gamma)\n",
    "        white_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        white_optimizer.step()\n",
    "\n",
    "    # Train the Black model\n",
    "    if len(black_replay_buffer) > batch_size:\n",
    "        batch = random.sample(black_replay_buffer, batch_size)\n",
    "        loss = compute_loss(batch, black_q_network, black_target_network, gamma)\n",
    "        black_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        black_optimizer.step()\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Update target networks\n",
    "    if episode % target_update_freq == 0:\n",
    "        white_target_network.load_state_dict(white_q_network.state_dict())\n",
    "        black_target_network.load_state_dict(black_q_network.state_dict())\n",
    "\n",
    "    # Log progress\n",
    "    print(f\"Episode: {episode + 1}, White Reward: {white_total_reward}, Black Reward: {black_total_reward}, Epsilon: {epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6349510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 11 19:43:08 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 5000                On  |   00000000:AF:00.0 Off |                  Off |\n",
      "| 33%   33C    P8              9W /  230W |     262MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     70583      C   ...1_stl29/.conda/envs/stl7/bin/python        258MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6edec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
