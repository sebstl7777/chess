{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "\n",
    "class ChessEnvironment:\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "        self.state_size = (12, 8, 8)\n",
    "        self.action_size = 8 * 8 * 8 * 8\n",
    "        self.current_player = chess.WHITE\n",
    "\n",
    "    def reset(self, fen=None):\n",
    "        if fen:\n",
    "            self.board.set_fen(fen)\n",
    "        else:\n",
    "            self.board.reset()\n",
    "        self.current_player = self.board.turn\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        state = np.zeros((13, 8, 8), dtype=np.float32)\n",
    "        for square in chess.SQUARES:\n",
    "            piece = self.board.piece_at(square)\n",
    "            if piece is not None:\n",
    "                channel = self._piece_to_channel(piece)\n",
    "                row, col = self._square_to_coords(square)\n",
    "                state[channel, row, col] = 1.0\n",
    "\n",
    "        state[12, :, :] = 1.0 if self.board.turn == chess.WHITE else 0.0\n",
    "\n",
    "        if self.current_player == chess.BLACK:\n",
    "            state = np.flip(state, axis=(1, 2))  # Flip board for Black perspective\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _piece_to_channel(self, piece):\n",
    "        piece_type = piece.piece_type - 1\n",
    "        color_offset = 0 if piece.color == chess.WHITE else 6\n",
    "        return piece_type + color_offset\n",
    "\n",
    "    def _square_to_coords(self, square):\n",
    "        row = 7 - chess.square_rank(square)\n",
    "        col = chess.square_file(square)\n",
    "        return row, col\n",
    "\n",
    "    def get_legal_moves(self):\n",
    "        legal_moves = []\n",
    "        for move in self.board.legal_moves:\n",
    "            row1, col1 = self._square_to_coords(move.from_square)\n",
    "            row2, col2 = self._square_to_coords(move.to_square)\n",
    "            legal_moves.append((row1, col1, row2, col2))\n",
    "        return legal_moves\n",
    "\n",
    "    def step(self, action):\n",
    "        row1, col1, row2, col2 = action\n",
    "        from_square = self._coords_to_square(row1, col1)\n",
    "        to_square = self._coords_to_square(row2, col2)\n",
    "\n",
    "        piece = self.board.piece_at(from_square)\n",
    "        if piece and piece.piece_type == chess.PAWN:\n",
    "            if (piece.color == chess.WHITE and row2 == 0) or (piece.color == chess.BLACK and row2 == 7):\n",
    "                move = chess.Move(from_square, to_square, promotion=chess.QUEEN)\n",
    "            else:\n",
    "                move = chess.Move(from_square, to_square)\n",
    "        else:\n",
    "            move = chess.Move(from_square, to_square)\n",
    "\n",
    "        if move not in self.board.legal_moves:\n",
    "            raise ValueError(f\"Illegal move attempted: {move}\")\n",
    "\n",
    "        self.board.push(move)\n",
    "        new_state = self._get_state()\n",
    "        done = self.board.is_game_over()\n",
    "        reward = self.calculate_reward(done)\n",
    "\n",
    "        return new_state, reward, done\n",
    "\n",
    "    def _coords_to_square(self, row, col):\n",
    "        return chess.square(col, 7 - row)\n",
    "\n",
    "    '''def calculate_reward(env, agent_color):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on the game outcome and the agent's color.\n",
    "        :param env: The ChessEnvironment object.\n",
    "        :param agent_color: The color the agent is playing as (chess.WHITE or chess.BLACK).\n",
    "        :return: The reward for the agent.\n",
    "        \"\"\"\n",
    "        if env.board.is_checkmate():\n",
    "            # Agent wins if it checkmates the opponent\n",
    "            if env.board.turn != agent_color:\n",
    "                return 1.0  # Agent wins\n",
    "            else:\n",
    "                return -1.0  # Agent loses\n",
    "        elif env.board.is_stalemate() or env.board.is_insufficient_material():\n",
    "            return 0.0  # Draw\n",
    "        else:\n",
    "            return 0.0  # Game is still ongoing\n",
    "    '''\n",
    "    \n",
    "    def calculate_reward(env, blah):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on the game outcome.\n",
    "        :param env: The ChessEnvironment object.\n",
    "        :return: A tuple (white_reward, black_reward).\n",
    "        \"\"\"\n",
    "        if env.board.is_checkmate():\n",
    "            # White wins if it's Black's turn (Black is checkmated)\n",
    "            if env.board.turn == chess.BLACK:\n",
    "                return [1.0, -1.0]  # White wins, Black loses\n",
    "            else:\n",
    "                return [-1.0, 1.0]  # Black wins, White loses\n",
    "        elif env.board.is_stalemate() or env.board.is_insufficient_material():\n",
    "            return [0.0, 0.0]  # Draw\n",
    "        else:\n",
    "            return [0.0, 0.0]  # Game is still ongoing\n",
    "           \n",
    "\n",
    "    def render(self):\n",
    "        print(self.board)\n",
    "\n",
    "# Keep everything else unchanged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhiteChessCNN(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(WhiteChessCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(13, 32, kernel_size=3, stride=1, padding=1)  # Input: 13 channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))  # Shape: [batch_size, 32, 8, 8]\n",
    "        x = torch.relu(self.conv2(x))  # Shape: [batch_size, 64, 8, 8]\n",
    "        x = x.view(x.size(0), -1)  # Flatten: [batch_size, 64 * 8 * 8]\n",
    "        x = torch.relu(self.fc1(x))  # Shape: [batch_size, 128]\n",
    "        return self.fc2(x)  # Shape: [batch_size, action_size]\n",
    "\n",
    "class BlackChessCNN(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(BlackChessCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(13, 32, kernel_size=3, stride=1, padding=1)  # Input: 13 channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))  # Shape: [batch_size, 32, 8, 8]\n",
    "        x = torch.relu(self.conv2(x))  # Shape: [batch_size, 64, 8, 8]\n",
    "        x = x.view(x.size(0), -1)  # Flatten: [batch_size, 64 * 8 * 8]\n",
    "        x = torch.relu(self.fc1(x))  # Shape: [batch_size, 128]\n",
    "        return self.fc2(x)  # Shape: [batch_size, action_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0bde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, q_network, env, epsilon):\n",
    "    \"\"\"Select an action using epsilon-greedy strategy, ensuring only legal moves are considered.\"\"\"\n",
    "    legal_moves = env.get_legal_moves()  # Get all legal moves as tuples (row1, col1, row2, col2)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        # Exploration: choose a random legal move\n",
    "        return random.choice(legal_moves)\n",
    "    else:\n",
    "        # Exploitation: choose the best legal move using the Q-network\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state).view(8, 8, 8, 8)\n",
    "\n",
    "        # Filter Q-values so only legal moves remain\n",
    "        legal_q_values = []\n",
    "        for move in legal_moves:\n",
    "            row1, col1, row2, col2 = move\n",
    "            legal_q_values.append((move, q_values[row1, col1, row2, col2].item()))\n",
    "\n",
    "        if not legal_q_values:\n",
    "            raise ValueError(\"No legal moves available â€” this shouldn't happen!\")\n",
    "\n",
    "        # Choose the legal move with the highest Q-value\n",
    "        best_move = max(legal_q_values, key=lambda x: x[1])[0]\n",
    "        return best_move\n",
    "\n",
    "def compute_loss(batch, q_network, target_network, gamma):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \"\"\"Compute the loss for a mini-batch of experiences.\"\"\"\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    # Reshape states and next_states to remove extra dimension\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32).squeeze(1).to(device)  # Remove extra dimension\n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32).squeeze(1).to(device)  # Remove extra dimension\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Compute Q-values for current states\n",
    "    q_values = q_network(states)\n",
    "\n",
    "    # Convert actions to a single index for gathering\n",
    "    action_indices = [np.ravel_multi_index(action, (8, 8, 8, 8)) for action in actions]  \n",
    "    # Convert to PyTorch tensor\n",
    "    action_indices = torch.tensor(action_indices, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    # Now gather the Q-values\n",
    "    q_values = q_values.gather(1, action_indices)\n",
    "\n",
    "    # Compute target Q-values\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_network(next_states)\n",
    "        next_q_values = next_q_values.max(dim=1)[0]\n",
    "        targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "    # Compute MSE loss\n",
    "    loss = nn.functional.mse_loss(q_values.squeeze(1), targets)  # squeeze to match dimensions\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and get a sample state\n",
    "env = ChessEnvironment()\n",
    "\n",
    "# Initialize the CNN\n",
    "action_size = 8 * 8 * 8 * 8  # Maximum number of possible moves\n",
    "\n",
    "white_q_network = WhiteChessCNN(action_size)\n",
    "white_target_network = WhiteChessCNN(action_size)\n",
    "white_target_network.load_state_dict(white_q_network.state_dict())\n",
    "white_target_network.eval()\n",
    "black_q_network = BlackChessCNN(action_size)\n",
    "black_target_network = BlackChessCNN(action_size)\n",
    "black_target_network.load_state_dict(black_q_network.state_dict())\n",
    "black_target_network.eval()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move models to GPU\n",
    "white_q_network.to(device)\n",
    "black_q_network.to(device)\n",
    "white_target_network.to(device)\n",
    "black_target_network.to(device)\n",
    "\n",
    "white_optimizer = optim.Adam(white_q_network.parameters(), lr=0.001)\n",
    "black_optimizer = optim.Adam(black_q_network.parameters(), lr=0.001)\n",
    "white_replay_buffer = deque(maxlen=100000)\n",
    "black_replay_buffer = deque(maxlen=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = .9  # Initial exploration rate\n",
    "epsilon_min = 0.01  # Minimum exploration rate\n",
    "epsilon_decay = 0.995  # Decay rate for exploration\n",
    "gamma = 0.99  # Discount factor\n",
    "batch_size = 64  # Mini-batch size\n",
    "target_update_freq = 100  # Frequency of updating the target network\n",
    "num_episodes = 100  # Number of episodes to train\n",
    "queens_gambit_fen = \"rnbqkbnr/ppp1pppp/8/3p4/2PP4/8/PP2PPPP/RNBQKBNR w KQkq - 0 3\"\n",
    "ruy_lopez_fen = \"r1bqkbnr/pppp1ppp/2n5/1B2p3/4P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 4\"\n",
    "italian_game_fen = \"r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 4\"\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "for episode in range(num_episodes):\n",
    "    env = ChessEnvironment()\n",
    "    state = env.reset(fen=italian_game_fen)\n",
    "    state = np.expand_dims(state, axis=0).copy()  # Add batch dimension\n",
    "    done = False\n",
    "    white_total_reward = 0\n",
    "    black_total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # White's turn\n",
    "        if env.board.turn == chess.WHITE:\n",
    "            action = select_action(torch.tensor(state, dtype=torch.float32).to(device), white_q_network, env, epsilon)\n",
    "            next_state, _, done = env.step(action)  # Immediate reward is 0 for non-terminal states\n",
    "            next_state = np.expand_dims(next_state, axis=0).copy()  # Add batch dimension\n",
    "\n",
    "            # Store experience in White's replay buffer\n",
    "            white_replay_buffer.append((state, action, 0.0, next_state, done))  # Immediate reward is 0\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "        # Black's turn\n",
    "        else:\n",
    "            action = select_action(torch.tensor(state, dtype=torch.float32).to(device), black_q_network, env, epsilon)\n",
    "            next_state, _, done = env.step(action)  # Immediate reward is 0 for non-terminal states\n",
    "            next_state = np.expand_dims(next_state, axis=0).copy()  # Add batch dimension\n",
    "\n",
    "            # Store experience in Black's replay buffer\n",
    "            black_replay_buffer.append((state, action, 0.0, next_state, done))  # Immediate reward is 0\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "    # Calculate the reward at the end of the game\n",
    "    if env.board.is_checkmate():\n",
    "        if env.board.turn == chess.BLACK:\n",
    "            white_reward, black_reward = 1.0, -1.0  # White wins, Black loses\n",
    "        else:\n",
    "            white_reward, black_reward = -1.0, 1.0  # Black wins, White loses\n",
    "    elif env.board.is_stalemate() or env.board.is_insufficient_material():\n",
    "        white_reward, black_reward = 0.0, 0.0  # Draw\n",
    "    else:\n",
    "        white_reward, black_reward = 0.0, 0.0  # Game is still ongoing (should not happen)\n",
    "\n",
    "    # Update the rewards in the replay buffers for terminal states\n",
    "    for i in range(len(white_replay_buffer)):\n",
    "        state, action, _, next_state, done = white_replay_buffer[i]\n",
    "        if done:\n",
    "            white_replay_buffer[i] = (state, action, white_reward, next_state, done)\n",
    "\n",
    "    for i in range(len(black_replay_buffer)):\n",
    "        state, action, _, next_state, done = black_replay_buffer[i]\n",
    "        if done:\n",
    "            black_replay_buffer[i] = (state, action, black_reward, next_state, done)\n",
    "\n",
    "    # Update total rewards\n",
    "    white_total_reward += white_reward\n",
    "    black_total_reward += black_reward\n",
    "\n",
    "    # Train the White model\n",
    "    if len(white_replay_buffer) > batch_size:\n",
    "        batch = random.sample(white_replay_buffer, batch_size)\n",
    "        loss = compute_loss(batch, white_q_network, white_target_network, gamma)\n",
    "        white_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        white_optimizer.step()\n",
    "\n",
    "    # Train the Black model\n",
    "    if len(black_replay_buffer) > batch_size:\n",
    "        batch = random.sample(black_replay_buffer, batch_size)\n",
    "        loss = compute_loss(batch, black_q_network, black_target_network, gamma)\n",
    "        black_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        black_optimizer.step()\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Update target networks\n",
    "    if episode % target_update_freq == 0:\n",
    "        white_target_network.load_state_dict(white_q_network.state_dict())\n",
    "        black_target_network.load_state_dict(black_q_network.state_dict())\n",
    "\n",
    "    # Log progress\n",
    "    print(f\"Episode: {episode + 1}, White Reward: {white_total_reward}, Black Reward: {black_total_reward}, Epsilon: {epsilon:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
